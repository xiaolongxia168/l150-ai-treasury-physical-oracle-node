#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¿æ¥åˆ°ç°æœ‰çš„ openclaw æµè§ˆå™¨è¿›è¡ŒæŠ“å–
"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

PROJECT_ROOT = Path(__file__).parent.parent.resolve()


async def find_tab_by_url(browser, url_pattern):
    """æŸ¥æ‰¾åŒ…å«æŒ‡å®š URL çš„æ ‡ç­¾é¡µ"""
    for context in browser.contexts:
        for page in context.pages:
            if url_pattern in page.url:
                logger.info(f"âœ“ æ‰¾åˆ°æ ‡ç­¾é¡µ: {page.url}")
                return page
    return None


async def discover_menu(page):
    """å‘ç°èœå• - æ”¹è¿›ç‰ˆï¼Œæ”¯æŒç‚¹å‡»å’Œé“¾æ¥"""
    menu_items = []

    # å°è¯•å¤šç§èœå•é€‰æ‹©å™¨
    selectors = [
        # ä¾§è¾¹æ é“¾æ¥
        'aside a', '.sidebar a', '[class*="sidebar"] a',
        'nav a', '.nav a', '[class*="nav"] a',
        '.menu a', '[class*="menu"] a',
        # å¯ç‚¹å‡»çš„èœå•é¡¹ï¼ˆå³ä½¿ä¸æ˜¯é“¾æ¥ï¼‰
        'aside [role="menuitem"]', '.sidebar [role="menuitem"]',
        'aside li', '.sidebar li', '[class*="sidebar"] li',
        'nav li', '.menu li'
    ]

    for selector in selectors:
        try:
            elements = await page.query_selector_all(selector)
            logger.info(f"  å°è¯•é€‰æ‹©å™¨ '{selector}': æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")

            for elem in elements[:50]:
                try:
                    # è·å–æ–‡æœ¬
                    text = (await elem.inner_text()).strip()
                    if not text or len(text) > 50 or text in ['', ' ']:
                        continue

                    # å°è¯•è·å– href
                    href = await elem.get_attribute('href')

                    # å¦‚æœæ²¡æœ‰ hrefï¼Œå°è¯•ä»å­å…ƒç´  a æ ‡ç­¾è·å–
                    if not href:
                        link = await elem.query_selector('a')
                        if link:
                            href = await link.get_attribute('href')

                    # å¦‚æœä»ç„¶æ²¡æœ‰ hrefï¼Œæ£€æŸ¥æ˜¯å¦å¯ç‚¹å‡»ï¼ˆdata-* å±æ€§ç­‰ï¼‰
                    if not href:
                        # æ£€æŸ¥ onclick æˆ–å…¶ä»–ç‚¹å‡»å±æ€§
                        onclick = await elem.get_attribute('onclick')
                        data_url = await elem.get_attribute('data-url')
                        if onclick or data_url:
                            # è¿™æ˜¯å¯ç‚¹å‡»å…ƒç´ ï¼Œä¿å­˜ä¸ºå…ƒç´ å¼•ç”¨
                            menu_items.append({
                                'text': text,
                                'element': elem,
                                'type': 'clickable'
                            })
                            continue

                    if href and href not in ['#', 'javascript:void(0)', 'javascript:;']:
                        # æ„å»ºå®Œæ•´ URL
                        if href.startswith('http'):
                            full_url = href
                        elif href.startswith('/'):
                            from urllib.parse import urljoin
                            full_url = urljoin(page.url, href)
                        else:
                            continue

                        # å»é‡
                        if not any(m.get('full_url') == full_url for m in menu_items):
                            menu_items.append({
                                'text': text,
                                'href': href,
                                'full_url': full_url,
                                'type': 'link'
                            })
                except Exception as e:
                    pass
        except Exception as e:
            pass

    logger.info(f"âœ“ å‘ç° {len(menu_items)} ä¸ªèœå•é¡¹ï¼ˆé“¾æ¥ + å¯ç‚¹å‡»ï¼‰")
    return menu_items[:25]  # é™åˆ¶ 25 ä¸ª


async def extract_page_data(page):
    """æå–é¡µé¢æ•°æ®"""
    data = {
        'title': await page.title(),
        'url': page.url,
        'timestamp': datetime.now().isoformat(),
        'tables': [],
        'stats': [],
        'text_content': []
    }

    # æå–è¡¨æ ¼
    tables = await page.query_selector_all('table')
    for table in tables[:5]:
        try:
            headers = []
            for cell in await table.query_selector_all('thead th, thead td'):
                text = (await cell.inner_text()).strip()
                if text:
                    headers.append(text)

            rows = []
            table_rows = await table.query_selector_all('tbody tr')
            for row in table_rows[:50]:
                row_data = []
                cells = await row.query_selector_all('td, th')
                for cell in cells:
                    text = (await cell.inner_text()).strip()
                    row_data.append(text)
                if row_data:
                    rows.append(row_data)

            if headers or rows:
                data['tables'].append({'headers': headers, 'rows': rows})
        except:
            pass

    # æå–ç»Ÿè®¡æ•°å­—
    for selector in [
        '.stat', '.metric', '.count', '.number',
        '[class*="data"]', '[class*="stat"]',
        '[class*="count"]', '[class*="metric"]'
    ]:
        try:
            for elem in (await page.query_selector_all(selector))[:10]:
                text = (await elem.inner_text()).strip()
                if text and len(text) < 100:
                    data['stats'].append(text)
        except:
            pass

    # æå–ä¸»è¦æ–‡æœ¬å†…å®¹
    try:
        main_selectors = ['main', '#app', '.content', '[class*="content"]']
        for selector in main_selectors:
            elem = await page.query_selector(selector)
            if elem:
                text = (await elem.inner_text()).strip()
                if text and len(text) > 50:
                    data['text_content'].append(text[:5000])  # é™åˆ¶é•¿åº¦
                    break
    except:
        pass

    return data


async def crawl_platform(browser, platform_name, url_pattern):
    """æŠ“å–å¹³å°æ•°æ®"""
    logger.info("=" * 60)
    logger.info(f"å¼€å§‹æŠ“å–: {platform_name}")
    logger.info(f"URL æ¨¡å¼: {url_pattern}")
    logger.info("=" * 60)

    # æŸ¥æ‰¾æ ‡ç­¾é¡µ
    page = await find_tab_by_url(browser, url_pattern)

    if not page:
        logger.warning(f"âš  æœªæ‰¾åˆ° {platform_name} çš„æ ‡ç­¾é¡µï¼Œè¯·ç¡®ä¿æµè§ˆå™¨ä¸­å·²æ‰“å¼€è¯¥é¡µé¢")
        return []

    all_data = []

    try:
        # ç­‰å¾…é¡µé¢åŠ è½½
        await page.wait_for_load_state('networkidle', timeout=10000)

        title = await page.title()
        current_url = page.url
        logger.info(f"âœ“ é¡µé¢æ ‡é¢˜: {title}")
        logger.info(f"âœ“ å½“å‰ URL: {current_url}")

        # æˆªå›¾
        screenshot_path = PROJECT_ROOT / 'logs' / f'{platform_name}_existing_{datetime.now().strftime("%H%M%S")}.png'
        screenshot_path.parent.mkdir(exist_ok=True)
        await page.screenshot(path=str(screenshot_path), full_page=True)
        logger.info(f"âœ“ æˆªå›¾: {screenshot_path}")

        # æå–å½“å‰é¡µé¢æ•°æ®
        logger.info("æå–å½“å‰é¡µé¢æ•°æ®...")
        data = await extract_page_data(page)
        data['menu_name'] = 'é¦–é¡µ'
        all_data.append(data)
        logger.info(f"  âœ“ æå–: {len(data.get('tables', []))} ä¸ªè¡¨æ ¼, {len(data.get('stats', []))} ä¸ªç»Ÿè®¡")

        # å‘ç°èœå•
        logger.info("\nå‘ç°èœå•...")
        menu_links = await discover_menu(page)

        if not menu_links:
            logger.warning("âš  æœªå‘ç°èœå•é“¾æ¥ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨æ»šåŠ¨é¡µé¢æˆ–ç­‰å¾…åŠ è½½")
            return all_data

        logger.info(f"\nâœ“ å‘ç° {len(menu_links)} ä¸ªèœå•å…¥å£:")
        for i, item in enumerate(menu_links, 1):
            if item['type'] == 'link':
                logger.info(f"  {i}. [{item['type']}] {item['text']} -> {item['full_url'][:80]}")
            else:
                logger.info(f"  {i}. [{item['type']}] {item['text']}")

        # é€ä¸ªæŠ“å–èœå•
        for i, item in enumerate(menu_links, 1):
            logger.info(f"\n[{i}/{len(menu_links)}] æŠ“å–èœå•: {item['text']}")

            try:
                # æ ¹æ®ç±»å‹å¤„ç†
                if item['type'] == 'link':
                    # å¯¼èˆªåˆ°èœå•é¡µé¢
                    await page.goto(item['full_url'], timeout=30000, wait_until='networkidle')
                elif item['type'] == 'clickable':
                    # ç‚¹å‡»å…ƒç´ 
                    await item['element'].click()

                await page.wait_for_timeout(2000)

                # æå–æ•°æ®
                data = await extract_page_data(page)
                data['menu_name'] = link['text']
                all_data.append(data)

                tables = len(data.get('tables', []))
                stats = len(data.get('stats', []))
                logger.info(f"  âœ“ æå–: {tables} ä¸ªè¡¨æ ¼, {stats} ä¸ªç»Ÿè®¡")

                # ç®€å•ç¿»é¡µï¼ˆæœ€å¤š 3 é¡µï¼‰
                for page_num in range(2, 5):
                    next_selectors = [
                        'button:has-text("ä¸‹ä¸€é¡µ")',
                        'a:has-text("ä¸‹ä¸€é¡µ")',
                        '.ant-pagination-next',
                        '[class*="next"]'
                    ]

                    next_btn = None
                    for selector in next_selectors:
                        next_btn = await page.query_selector(selector)
                        if next_btn:
                            # æ£€æŸ¥æ˜¯å¦ç¦ç”¨
                            is_disabled = await next_btn.is_disabled() if hasattr(next_btn, 'is_disabled') else False
                            class_name = await next_btn.get_attribute('class') or ''
                            if not is_disabled and 'disabled' not in class_name:
                                break
                            next_btn = None

                    if not next_btn:
                        break

                    await next_btn.click()
                    await page.wait_for_timeout(2000)

                    data = await extract_page_data(page)
                    data['menu_name'] = f"{link['text']} (ç¬¬{page_num}é¡µ)"
                    all_data.append(data)
                    logger.info(f"    âœ“ ç¬¬ {page_num} é¡µ")

            except Exception as e:
                logger.error(f"  âœ— å¤±è´¥: {e}")

        logger.info(f"\nâœ“ æŠ“å–å®Œæˆï¼å…± {len(all_data)} ä¸ªé¡µé¢")

    except Exception as e:
        logger.error(f"æŠ“å–å¤±è´¥: {e}", exc_info=True)

    return all_data


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è¿æ¥åˆ° openclaw æµè§ˆå™¨è¿›è¡ŒæŠ“å–")
    print("=" * 60)

    async with async_playwright() as p:
        try:
            # è¿æ¥åˆ°ç°æœ‰çš„ Chrome å®ä¾‹
            logger.info("è¿æ¥åˆ° openclaw æµè§ˆå™¨ï¼ˆç«¯å£ 18800ï¼‰...")
            browser = await p.chromium.connect_over_cdp('http://localhost:18800')
            logger.info(f"âœ“ å·²è¿æ¥ï¼Œå…± {len(browser.contexts)} ä¸ªä¸Šä¸‹æ–‡")

            all_results = {}

            # æŠ“å–æŠ–éŸ³æ¥å®¢ï¼ˆä½¿ç”¨æ­£ç¡®çš„ URLï¼‰
            print("\nã€1/2ã€‘æŠ“å–æŠ–éŸ³æ¥å®¢...")
            douyin_data = await crawl_platform(browser, 'douyin_laike', 'douyin.com')
            all_results['douyin_laike'] = douyin_data

            print("\n" + "="*60)
            print("ç­‰å¾… 5 ç§’...")
            await asyncio.sleep(5)

            # æŠ“å–ç¾å›¢å¼€åº—å®ï¼ˆä½¿ç”¨æ­£ç¡®çš„ URLï¼‰
            print("\nã€2/2ã€‘æŠ“å–ç¾å›¢å¼€åº—å®...")
            meituan_data = await crawl_platform(browser, 'meituan_kaidian', 'e.dianping.com')
            all_results['meituan_kaidian'] = meituan_data

            # ä¿å­˜æ‰€æœ‰æ•°æ®
            for platform_name, data in all_results.items():
                if data:
                    output_path = PROJECT_ROOT / 'data' / f'{platform_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
                    output_path.parent.mkdir(exist_ok=True)

                    with open(output_path, 'w', encoding='utf-8') as f:
                        json.dump(data, f, indent=2, ensure_ascii=False)

                    size = output_path.stat().st_size / 1024
                    logger.info(f"âœ“ {platform_name} æ•°æ®å·²ä¿å­˜: {output_path} ({size:.2f} KB)")

            print("\n" + "="*60)
            print(f"ğŸ‰ å…¨éƒ¨æŠ“å–å®Œæˆï¼")
            print("="*60)
            print(f"\nç»Ÿè®¡:")
            print(f"  æŠ–éŸ³æ¥å®¢: {len(douyin_data)} ä¸ªé¡µé¢")
            print(f"  ç¾å›¢å¼€åº—å®: {len(meituan_data)} ä¸ªé¡µé¢")
            print(f"\næŸ¥çœ‹ç»“æœï¼š")
            print(f"  ls -lh {PROJECT_ROOT / 'data'}")

            # æ–­å¼€è¿æ¥ï¼ˆä¸å…³é—­æµè§ˆå™¨ï¼‰
            await browser.close()

        except Exception as e:
            logger.error(f"è¿æ¥å¤±è´¥: {e}", exc_info=True)
            print("\nâš  æç¤ºï¼š")
            print("1. ç¡®ä¿ openclaw æµè§ˆå™¨æ­£åœ¨è¿è¡Œ")
            print("2. ç¡®ä¿å·²ç»åœ¨æµè§ˆå™¨ä¸­ç™»å½•äº†æŠ–éŸ³æ¥å®¢å’Œç¾å›¢å¼€åº—å®")
            print("3. ç¡®ä¿æµè§ˆå™¨æ ‡ç­¾é¡µå·²æ‰“å¼€è¿™ä¸¤ä¸ªç½‘ç«™")


if __name__ == '__main__':
    asyncio.run(main())
