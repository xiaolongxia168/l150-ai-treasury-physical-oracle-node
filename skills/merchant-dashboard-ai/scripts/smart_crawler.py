#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½çˆ¬è™« - è‡ªåŠ¨å‘ç°å¹¶ç‚¹å‡»æ‰€æœ‰èœå•é¡¹
"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

PROJECT_ROOT = Path(__file__).parent.parent.resolve()


async def find_all_clickable_menu_items(page):
    """æ™ºèƒ½å‘ç°æ‰€æœ‰å¯ç‚¹å‡»çš„èœå•é¡¹"""
    logger.info("å¼€å§‹æ™ºèƒ½èœå•å‘ç°...")

    # æ‰§è¡Œ JavaScript åœ¨æµè§ˆå™¨ä¸­æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„èœå•é¡¹
    menu_items = await page.evaluate("""
        () => {
            const items = [];
            const seen = new Set();

            // æŸ¥æ‰¾å·¦ä¾§åŒºåŸŸçš„æ‰€æœ‰å¯èƒ½å…ƒç´ 
            const leftElements = Array.from(document.querySelectorAll('*')).filter(el => {
                const rect = el.getBoundingClientRect();
                // å·¦ä¾§ 300px ä»¥å†…ï¼Œæœ‰æ–‡æœ¬å†…å®¹ï¼Œå¯è§
                return rect.left < 300 &&
                       rect.width > 0 &&
                       rect.height > 0 &&
                       rect.height < 100 &&  // ä¸è¦å¤ªé«˜ï¼ˆé¿å…å®¹å™¨ï¼‰
                       el.textContent &&
                       el.textContent.trim().length > 0 &&
                       el.textContent.trim().length < 50;
            });

            leftElements.forEach(el => {
                const text = el.textContent.trim();
                const tag = el.tagName.toLowerCase();

                // è·³è¿‡å·²è§è¿‡çš„æ–‡æœ¬æˆ–åŒ…å«æ¢è¡Œçš„
                if (seen.has(text) || text === '' || text.includes('\\n')) {
                    return;
                }

                // æ£€æŸ¥æ˜¯å¦å¯ç‚¹å‡»
                const style = window.getComputedStyle(el);
                const hasClickCursor = style.cursor === 'pointer';
                const hasOnClick = el.onclick !== null;

                // æ£€æŸ¥çˆ¶å…ƒç´ æ˜¯å¦å¯ç‚¹å‡»
                let parent = el.parentElement;
                let parentClickable = false;
                for (let i = 0; i < 3 && parent; i++) {
                    const parentStyle = window.getComputedStyle(parent);
                    if (parentStyle.cursor === 'pointer' || parent.onclick !== null) {
                        parentClickable = true;
                        break;
                    }
                    parent = parent.parentElement;
                }

                if (hasClickCursor || hasOnClick || parentClickable || tag === 'a' || tag === 'button') {
                    seen.add(text);
                    items.push({text: text, tag: tag});
                }
            });

            return items;
        }
    """)

    logger.info(f"âœ“ å‘ç° {len(menu_items)} ä¸ªå¯èƒ½çš„èœå•é¡¹")
    return menu_items


async def extract_page_data(page, page_name):
    """æå–é¡µé¢æ•°æ®"""
    try:
        data = {
            'name': page_name,
            'title': await page.title(),
            'url': page.url,
            'timestamp': datetime.now().isoformat(),
            'content': {}
        }

        # æå–é¡µé¢æ–‡æœ¬å†…å®¹
        content = await page.evaluate("""
            () => {
                const selectors = ['main', '#app', '[class*="content"]', 'body'];
                for (const sel of selectors) {
                    const elem = document.querySelector(sel);
                    if (elem) {
                        const text = elem.innerText || elem.textContent;
                        if (text && text.length > 100) {
                            return text.substring(0, 10000);
                        }
                    }
                }
                return '';
            }
        """)

        data['content']['text'] = content

        # æå–æ‰€æœ‰è¡¨æ ¼
        tables = await page.evaluate("""
            () => {
                const tables = [];
                document.querySelectorAll('table').forEach(table => {
                    const headers = Array.from(table.querySelectorAll('thead th, thead td')).map(th => th.innerText.trim());
                    const rows = Array.from(table.querySelectorAll('tbody tr')).slice(0, 50).map(tr => {
                        return Array.from(tr.querySelectorAll('td, th')).map(td => td.innerText.trim());
                    });
                    if (headers.length > 0 || rows.length > 0) {
                        tables.push({headers, rows});
                    }
                });
                return tables;
            }
        """)

        data['content']['tables'] = tables

        logger.info(f"  âœ“ æå–: {len(tables)} ä¸ªè¡¨æ ¼, æ–‡æœ¬ {len(content)} å­—ç¬¦")
        return data

    except Exception as e:
        logger.error(f"  âœ— æå–æ•°æ®å¤±è´¥: {e}")
        return None


async def crawl_platform(browser, platform_name, url_pattern):
    """æ™ºèƒ½æŠ“å–å¹³å°"""
    logger.info("=" * 60)
    logger.info(f"æ™ºèƒ½æŠ“å–: {platform_name}")
    logger.info(f"URL æ¨¡å¼: {url_pattern}")
    logger.info("=" * 60)

    # æŸ¥æ‰¾æ ‡ç­¾é¡µ
    page = None
    for context in browser.contexts:
        for p in context.pages:
            if url_pattern in p.url:
                page = p
                logger.info(f"âœ“ æ‰¾åˆ°æ ‡ç­¾é¡µ: {p.url}")
                break
        if page:
            break

    if not page:
        logger.warning(f"âš  æœªæ‰¾åˆ° {platform_name} çš„æ ‡ç­¾é¡µ")
        return []

    all_data = []

    try:
        # ç­‰å¾…é¡µé¢åŠ è½½
        await page.wait_for_load_state('domcontentloaded', timeout=10000)

        title = await page.title()
        logger.info(f"âœ“ é¡µé¢æ ‡é¢˜: {title}")

        # æˆªå›¾é¦–é¡µ
        screenshot_path = PROJECT_ROOT / 'logs' / f'{platform_name}_home_{datetime.now().strftime("%H%M%S")}.png'
        screenshot_path.parent.mkdir(exist_ok=True)
        await page.screenshot(path=str(screenshot_path), full_page=True)
        logger.info(f"âœ“ æˆªå›¾: {screenshot_path}")

        # æå–é¦–é¡µæ•°æ®
        home_data = await extract_page_data(page, 'é¦–é¡µ')
        if home_data:
            all_data.append(home_data)

        # æ™ºèƒ½å‘ç°èœå•
        menu_items = await find_all_clickable_menu_items(page)

        if not menu_items:
            logger.warning("âš  æœªå‘ç°èœå•é¡¹")
            return all_data

        logger.info(f"\nèœå•é¡¹åˆ—è¡¨:")
        for i, item in enumerate(menu_items[:30], 1):
            logger.info(f"  {i}. {item['text']}")

        # é€ä¸ªç‚¹å‡»èœå•é¡¹
        clicked = set()
        for i, item in enumerate(menu_items[:20], 1):
            if item['text'] in clicked:
                continue

            logger.info(f"\n[{i}/20] ç‚¹å‡»èœå•: {item['text']}")

            try:
                element = page.locator(f"text={item['text']}").first
                await element.click(timeout=5000)
                clicked.add(item['text'])
                await page.wait_for_timeout(2000)

                # æå–æ•°æ®
                data = await extract_page_data(page, item['text'])
                if data:
                    all_data.append(data)

                # ç®€å•æˆªå›¾
                if i <= 10:
                    ss_path = PROJECT_ROOT / 'logs' / f'{platform_name}_menu{i}_{datetime.now().strftime("%H%M%S")}.png'
                    await page.screenshot(path=str(ss_path))

            except Exception as e:
                logger.error(f"  âœ— å¤±è´¥: {e}")

        logger.info(f"\nâœ“ å…±æŠ“å– {len(all_data)} ä¸ªé¡µé¢")

    except Exception as e:
        logger.error(f"æŠ“å–å¤±è´¥: {e}", exc_info=True)

    return all_data


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– æ™ºèƒ½çˆ¬è™« - è‡ªåŠ¨å‘ç°èœå•å¹¶æŠ“å–")
    print("=" * 60)

    async with async_playwright() as p:
        try:
            logger.info("è¿æ¥åˆ° openclaw æµè§ˆå™¨ï¼ˆç«¯å£ 18800ï¼‰...")
            browser = await p.chromium.connect_over_cdp('http://localhost:18800')
            logger.info(f"âœ“ å·²è¿æ¥\n")

            all_results = {}

            # æŠ“å–æŠ–éŸ³æ¥å®¢
            print("\nã€1/2ã€‘æ™ºèƒ½æŠ“å–æŠ–éŸ³æ¥å®¢...")
            douyin_data = await crawl_platform(browser, 'douyin_laike', 'douyin.com')
            all_results['douyin_laike'] = douyin_data

            print("\n" + "="*60)
            print("ç­‰å¾… 5 ç§’...")
            await asyncio.sleep(5)

            # æŠ“å–ç¾å›¢
            print("\nã€2/2ã€‘æ™ºèƒ½æŠ“å–ç¾å›¢å¼€åº—å®...")
            meituan_data = await crawl_platform(browser, 'meituan_kaidian', 'dianping.com')
            all_results['meituan_kaidian'] = meituan_data

            # ä¿å­˜æ•°æ®
            for platform_name, data in all_results.items():
                if data:
                    output_path = PROJECT_ROOT / 'data' / f'{platform_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
                    output_path.parent.mkdir(exist_ok=True)

                    with open(output_path, 'w', encoding='utf-8') as f:
                        json.dump(data, f, indent=2, ensure_ascii=False)

                    size = output_path.stat().st_size / 1024
                    logger.info(f"âœ“ {platform_name} æ•°æ®å·²ä¿å­˜: {output_path} ({size:.2f} KB)")

            print("\n" + "="*60)
            print("ğŸ‰ æ™ºèƒ½æŠ“å–å®Œæˆï¼")
            print("="*60)
            print(f"\nç»Ÿè®¡:")
            print(f"  æŠ–éŸ³æ¥å®¢: {len(douyin_data)} ä¸ªé¡µé¢")
            print(f"  ç¾å›¢å¼€åº—å®: {len(meituan_data)} ä¸ªé¡µé¢")
            print(f"\næŸ¥çœ‹æ•°æ®ï¼š")
            print(f"  ls -lh {PROJECT_ROOT / 'data'}")

            await browser.close()

        except Exception as e:
            logger.error(f"å¤±è´¥: {e}", exc_info=True)


if __name__ == '__main__':
    asyncio.run(main())
