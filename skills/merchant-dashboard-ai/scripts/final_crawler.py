#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆç‰ˆæ™ºèƒ½çˆ¬è™« - ä½¿ç”¨æå–çš„ Cookie + ä»£ç†
"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

# è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆè„šæœ¬åœ¨ scripts/ ä¸‹ï¼Œæ‰€ä»¥ä¸Šä¸€çº§æ˜¯æ ¹ç›®å½•ï¼‰
PROJECT_ROOT = Path(__file__).parent.parent.resolve()


async def crawl_merchant_platform(platform_name, start_url, cookie_file):
    """çˆ¬å–å•†å®¶å¹³å°"""
    logger.info("=" * 60)
    logger.info(f"å¼€å§‹æŠ“å–: {platform_name}")
    logger.info(f"URL: {start_url}")
    logger.info("=" * 60)

    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    cookie_path = Path(cookie_file)
    if not cookie_path.is_absolute():
        cookie_path = PROJECT_ROOT / cookie_file

    logger.info(f"Cookie æ–‡ä»¶è·¯å¾„: {cookie_path}")

    # åŠ è½½ cookies
    if not cookie_path.exists():
        logger.error(f"Cookie æ–‡ä»¶ä¸å­˜åœ¨: {cookie_path}")
        logger.error(f"å½“å‰å·¥ä½œç›®å½•: {Path.cwd()}")
        logger.error(f"é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")
        return

    with open(cookie_path, 'r') as f:
        cookies = json.load(f)
    logger.info(f"âœ“ å·²åŠ è½½ {len(cookies)} ä¸ª Cookie")

    all_data = []

    async with async_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨ï¼ˆä½¿ç”¨ä»£ç†ï¼‰
        browser = await p.chromium.launch(
            headless=False,  # å¯è§†åŒ–æ¨¡å¼ï¼Œæ–¹ä¾¿è§‚å¯Ÿ
            proxy={'server': 'http://127.0.0.1:7897'}
        )

        context = await browser.new_context()
        await context.add_cookies(cookies)

        page = await context.new_page()

        try:
            # è®¿é—®é¦–é¡µ
            logger.info(f"æ­£åœ¨è®¿é—®: {start_url}")
            await page.goto(start_url, timeout=90000)
            await page.wait_for_timeout(8000)  # ç­‰å¾…åŠ è½½

            title = await page.title()
            logger.info(f"âœ“ é¡µé¢æ ‡é¢˜: {title}")

            # æˆªå›¾ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
            screenshot_path = PROJECT_ROOT / 'logs' / f'{platform_name}_homepage.png'
            screenshot_path.parent.mkdir(exist_ok=True)
            await page.screenshot(path=str(screenshot_path), full_page=True)
            logger.info(f"âœ“ æˆªå›¾: {screenshot_path}")

            # æ£€æŸ¥æ˜¯å¦ç™»å½•
            content = await page.content()
            if 'ç™»å½•' in title or 'login' in title.lower():
                logger.warning("âš  å¯èƒ½éœ€è¦ç™»å½•ï¼Œè¯·æ£€æŸ¥æˆªå›¾")
                logger.warning("âš  ç­‰å¾… 5 ç§’åç»§ç»­å°è¯•æŠ“å–...")
                await page.wait_for_timeout(5000)

            # å‘ç°å¹¶æŠ“å–èœå•
            menu_links = await discover_menu(page)
            logger.info(f"\nâœ“ å‘ç° {len(menu_links)} ä¸ªèœå•å…¥å£:")
            for i, link in enumerate(menu_links, 1):
                logger.info(f"  {i}. {link['text']}")

            # é€ä¸ªæŠ“å–
            for i, link in enumerate(menu_links, 1):
                logger.info(f"\n[{i}/{len(menu_links)}] æŠ“å–èœå•: {link['text']}")

                try:
                    await page.goto(link['full_url'], timeout=60000)
                    await page.wait_for_timeout(3000)

                    # æå–æ•°æ®
                    data = await extract_page_data(page)
                    data['menu_name'] = link['text']
                    data['url'] = link['full_url']
                    all_data.append(data)

                    tables = len(data.get('tables', []))
                    stats = len(data.get('stats', []))
                    logger.info(f"  âœ“ æå–: {tables} ä¸ªè¡¨æ ¼, {stats} ä¸ªç»Ÿè®¡")

                    # ç®€å•ç¿»é¡µï¼ˆæœ€å¤š3é¡µï¼‰
                    for page_num in range(2, 5):
                        next_btn = await page.query_selector('button:has-text("ä¸‹ä¸€é¡µ"), a:has-text("ä¸‹ä¸€é¡µ")')
                        if not next_btn:
                            break

                        await next_btn.click()
                        await page.wait_for_timeout(2000)

                        data = await extract_page_data(page)
                        data['menu_name'] = f"{link['text']} (ç¬¬{page_num}é¡µ)"
                        all_data.append(data)
                        logger.info(f"    âœ“ ç¬¬ {page_num} é¡µ")

                except Exception as e:
                    logger.error(f"  âœ— å¤±è´¥: {e}")

            # ä¿å­˜æ•°æ®ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
            if all_data:
                output_path = PROJECT_ROOT / 'data' / f'{platform_name}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
                output_path.parent.mkdir(exist_ok=True)

                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(all_data, f, indent=2, ensure_ascii=False)

                size = output_path.stat().st_size / 1024
                logger.info(f"\nâœ“ æ•°æ®å·²ä¿å­˜: {output_path} ({size:.2f} KB)")

            logger.info(f"\nâœ“ æŠ“å–å®Œæˆï¼å…± {len(all_data)} ä¸ªé¡µé¢")

        except Exception as e:
            logger.error(f"æŠ“å–å¤±è´¥: {e}", exc_info=True)
        finally:
            await browser.close()


async def discover_menu(page):
    """å‘ç°èœå•"""
    menu_links = []
    selectors = ['nav a', '.sidebar a', '.menu a', '[class*="nav"] a', 'aside a']

    for selector in selectors:
        try:
            links = await page.query_selector_all(selector)
            for link in links[:30]:
                try:
                    href = await link.get_attribute('href')
                    text = (await link.inner_text()).strip()

                    if href and text and len(text) < 30:
                        if href.startswith('http'):
                            full_url = href
                        elif href.startswith('/'):
                            from urllib.parse import urljoin
                            full_url = urljoin(page.url, href)
                        else:
                            continue

                        if not any(m['full_url'] == full_url for m in menu_links):
                            menu_links.append({'text': text, 'href': href, 'full_url': full_url})
                except:
                    pass
        except:
            pass

    return menu_links[:15]  # é™åˆ¶ 15 ä¸ª


async def extract_page_data(page):
    """æå–é¡µé¢æ•°æ®"""
    data = {'title': await page.title(), 'timestamp': datetime.now().isoformat(), 'tables': [], 'stats': []}

    # æå–è¡¨æ ¼
    tables = await page.query_selector_all('table')
    for table in tables[:3]:
        try:
            headers = []
            for cell in await table.query_selector_all('thead th, thead td'):
                headers.append((await cell.inner_text()).strip())

            rows = []
            for row in await table.query_selector_all('tbody tr')[:30]:
                row_data = []
                for cell in await row.query_selector_all('td, th'):
                    row_data.append((await cell.inner_text()).strip())
                if row_data:
                    rows.append(row_data)

            if headers or rows:
                data['tables'].append({'headers': headers, 'rows': rows})
        except:
            pass

    # æå–ç»Ÿè®¡
    for selector in ['.stat', '.metric', '.count', '.number', '[class*="data"]']:
        try:
            for elem in (await page.query_selector_all(selector))[:5]:
                text = (await elem.inner_text()).strip()
                if text and len(text) < 50:
                    data['stats'].append(text)
        except:
            pass

    return data


async def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("å•†å®¶åå°æ™ºèƒ½çˆ¬è™«")
    print("=" * 60)
    print("\n1. æŠ–éŸ³æ¥å®¢")
    print("2. ç¾å›¢å¼€åº—å®")
    print("3. ä¸¤ä¸ªéƒ½æŠ“å–")

    choice = input("\nè¯·é€‰æ‹© (1/2/3): ").strip()

    platforms = []
    if choice == '1':
        platforms = [('douyin_laike', 'https://laike.douyin.com/', 'cookies/douyin_laike.json')]
    elif choice == '2':
        platforms = [('meituan_kaidian', 'https://e.dianping.com/', 'cookies/meituan_kaidian.json')]
    elif choice == '3':
        platforms = [
            ('douyin_laike', 'https://laike.douyin.com/', 'cookies/douyin_laike.json'),
            ('meituan_kaidian', 'https://e.dianping.com/', 'cookies/meituan_kaidian.json')
        ]
    else:
        print("æ— æ•ˆé€‰æ‹©")
        return

    for name, url, cookie_file in platforms:
        await crawl_merchant_platform(name, url, cookie_file)
        if len(platforms) > 1:
            print("\n" + "=" * 60)
            print("3ç§’åç»§ç»­ä¸‹ä¸€ä¸ªå¹³å°...")
            print("=" * 60)
            await asyncio.sleep(3)

    print("\n" + "=" * 60)
    print("ğŸ‰ å…¨éƒ¨æŠ“å–å®Œæˆï¼")
    print("=" * 60)


if __name__ == '__main__':
    asyncio.run(main())
