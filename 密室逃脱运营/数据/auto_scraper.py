#!/usr/bin/env python3
"""
Playwrightè‡ªåŠ¨åŒ–æ•°æ®æŠ“å–è„šæœ¬
è¿è¡Œå‰è¯·ç¡®ä¿å·²å®‰è£…: pip install playwright
å¹¶å®‰è£…æµè§ˆå™¨: playwright install
"""

import asyncio
import json
import csv
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright

class AutoScraper:
    def __init__(self):
        self.data_dir = Path.home() / ".openclaw/workspace/å¯†å®¤é€ƒè„±è¿è¥/æ•°æ®"
        self.results = {}
        
    async def scrape_douyin(self, page):
        """æŠ“å–æŠ–éŸ³æ¥å®¢æ•°æ®"""
        print("ğŸµ æŠ“å–æŠ–éŸ³æ•°æ®...")
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        await page.wait_for_load_state('networkidle')
        
        # æŠ“å–è§†é¢‘æ•°æ®
        video_data = []
        try:
            # ç‚¹å‡»æ•°æ®èœå•
            await page.click('text=æ•°æ®')
            await page.wait_for_timeout(2000)
            
            # ç‚¹å‡»è§†é¢‘åˆ†æ
            await page.click('text=è§†é¢‘åˆ†æ')
            await page.wait_for_timeout(3000)
            
            # æå–è§†é¢‘åˆ—è¡¨æ•°æ®
            videos = await page.query_selector_all('.video-item')  # éœ€è¦æ ¹æ®å®é™…é¡µé¢è°ƒæ•´é€‰æ‹©å™¨
            
            for video in videos[:20]:  # æŠ“å–å‰20æ¡
                try:
                    title = await video.query_selector_eval('.video-title', 'el => el.textContent')
                    plays = await video.query_selector_eval('.play-count', 'el => el.textContent')
                    likes = await video.query_selector_eval('.like-count', 'el => el.textContent')
                    
                    video_data.append({
                        'title': title,
                        'plays': plays,
                        'likes': likes,
                        'scraped_at': datetime.now().isoformat()
                    })
                except:
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ æŠ–éŸ³æ•°æ®æŠ“å–éƒ¨åˆ†å¤±è´¥: {e}")
            
        self.results['douyin_videos'] = video_data
        return video_data
    
    async def scrape_meituan(self, page):
        """æŠ“å–ç¾å›¢å¼€åº—å®æ•°æ®"""
        print("ğŸœ æŠ“å–ç¾å›¢æ•°æ®...")
        
        await page.wait_for_load_state('networkidle')
        
        meituan_data = []
        try:
            # ç‚¹å‡»ç»è¥åˆ†æ
            await page.click('text=ç»è¥åˆ†æ')
            await page.wait_for_timeout(2000)
            
            # æŠ“å–äº¤æ˜“æ•°æ®
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´
            
        except Exception as e:
            print(f"âš ï¸ ç¾å›¢æ•°æ®æŠ“å–éƒ¨åˆ†å¤±è´¥: {e}")
            
        self.results['meituan'] = meituan_data
        return meituan_data
    
    def save_results(self):
        """ä¿å­˜æŠ“å–ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜ä¸ºJSON
        json_file = self.data_dir / f"scraped_data_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
            
        # ä¿å­˜ä¸ºCSVï¼ˆå¦‚æœæœ‰è§†é¢‘æ•°æ®ï¼‰
        if 'douyin_videos' in self.results:
            csv_file = self.data_dir / f"douyin_videos_{timestamp}.csv"
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                if self.results['douyin_videos']:
                    writer = csv.DictWriter(f, fieldnames=self.results['douyin_videos'][0].keys())
                    writer.writeheader()
                    writer.writerows(self.results['douyin_videos'])
                    
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {self.data_dir}")
        return json_file

async def main():
    scraper = AutoScraper()
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)  # å¯è§æ¨¡å¼ä¾¿äºè°ƒè¯•
        
        # æŠ“å–æŠ–éŸ³
        print("\nğŸŒ æ‰“å¼€æŠ–éŸ³æ¥å®¢...")
        page = await browser.new_page()
        await page.goto('https://e.douyin.com/')
        print("â³ è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼Œç„¶åæŒ‰å›è½¦ç»§ç»­...")
        input()
        
        await scraper.scrape_douyin(page)
        
        # æŠ“å–ç¾å›¢
        print("\nğŸŒ æ‰“å¼€ç¾å›¢å¼€åº—å®...")
        page2 = await browser.new_page()
        await page2.goto('https://e.waimai.meituan.com/')
        print("â³ è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼Œç„¶åæŒ‰å›è½¦ç»§ç»­...")
        input()
        
        await scraper.scrape_meituan(page2)
        
        # ä¿å­˜ç»“æœ
        scraper.save_results()
        
        await browser.close()
        print("\nâœ… æ•°æ®æŠ“å–å®Œæˆï¼")

if __name__ == '__main__':
    asyncio.run(main())
