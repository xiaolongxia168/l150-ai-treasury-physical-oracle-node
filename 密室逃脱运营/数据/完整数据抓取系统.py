#!/usr/bin/env python3
"""
å¯†å®¤é€ƒè„±è¿è¥æ•°æ®æŠ“å–ç³»ç»Ÿ
è‡ªåŠ¨æŠ“å–æŠ–éŸ³æ¥å®¢ + ç¾å›¢å¼€åº—å®æ•°æ®
"""

import json
import csv
import time
from datetime import datetime, timedelta
from pathlib import Path
import re

class DataScraper:
    def __init__(self):
        self.data_dir = Path.home() / ".openclaw/workspace/å¯†å®¤é€ƒè„±è¿è¥/æ•°æ®"
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
    def scrape_douyin_data(self):
        """
        æŠ–éŸ³æ¥å®¢æ•°æ®æŠ“å–
        éœ€è¦æ‰‹åŠ¨ç™»å½•åè¿è¡Œï¼Œæˆ–æä¾›Cookie
        """
        print("ğŸµ å¼€å§‹æŠ“å–æŠ–éŸ³æ¥å®¢æ•°æ®...")
        
        # æ•°æ®æŠ“å–é…ç½®
        douyin_config = {
            "login_url": "https://e.douyin.com/",
            "data_pages": [
                "/data/shop",
                "/data/video", 
                "/data/live",
                "/data/fans"
            ],
            "extract_fields": {
                "video_data": [
                    "publish_time", "title", "play_count", "like_count",
                    "comment_count", "share_count", "completion_rate",
                    "product_click", "order_count", "gmv"
                ],
                "fan_data": [
                    "date", "total_fans", "new_fans", "fan_profile", "active_time"
                ],
                "live_data": [
                    "date", "duration", "viewers", "interaction_rate", "conversion"
                ],
                "conversion_data": [
                    "date", "gmv", "orders", "verification_rate", "avg_price"
                ]
            }
        }
        
        # ä¿å­˜é…ç½®
        config_file = self.data_dir / "douyin_scraper_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(douyin_config, f, ensure_ascii=False, indent=2)
            
        print(f"âœ… æŠ–éŸ³æŠ“å–é…ç½®å·²ä¿å­˜: {config_file}")
        return douyin_config
    
    def scrape_meituan_data(self):
        """
        ç¾å›¢å¼€åº—å®æ•°æ®æŠ“å–
        """
        print("ğŸœ å¼€å§‹æŠ“å–ç¾å›¢å¼€åº—å®æ•°æ®...")
        
        meituan_config = {
            "login_url": "https://e.waimai.meituan.com/",
            "data_pages": [
                "/data/flow",
                "/data/trade",
                "/data/evaluate",
                "/data/compete"
            ],
            "extract_fields": {
                "flow_data": [
                    "date", "exposure", "visit", "click_rate", "conversion"
                ],
                "trade_data": [
                    "date", "orders", "revenue", "avg_price", "refund_rate"
                ],
                "evaluate_data": [
                    "date", "rating", "positive", "negative", "keywords"
                ],
                "compete_data": [
                    "date", "rank", "flow_source", "competitor_activity"
                ]
            }
        }
        
        config_file = self.data_dir / "meituan_scraper_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(meituan_config, f, ensure_ascii=False, indent=2)
            
        print(f"âœ… ç¾å›¢æŠ“å–é…ç½®å·²ä¿å­˜: {config_file}")
        return meituan_config
    
    def create_scraper_script(self):
        """
        åˆ›å»ºPlaywrightè‡ªåŠ¨åŒ–æŠ“å–è„šæœ¬
        """
        scraper_code = '''#!/usr/bin/env python3
"""
Playwrightè‡ªåŠ¨åŒ–æ•°æ®æŠ“å–è„šæœ¬
è¿è¡Œå‰è¯·ç¡®ä¿å·²å®‰è£…: pip install playwright
å¹¶å®‰è£…æµè§ˆå™¨: playwright install
"""

import asyncio
import json
import csv
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright

class AutoScraper:
    def __init__(self):
        self.data_dir = Path.home() / ".openclaw/workspace/å¯†å®¤é€ƒè„±è¿è¥/æ•°æ®"
        self.results = {}
        
    async def scrape_douyin(self, page):
        """æŠ“å–æŠ–éŸ³æ¥å®¢æ•°æ®"""
        print("ğŸµ æŠ“å–æŠ–éŸ³æ•°æ®...")
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        await page.wait_for_load_state('networkidle')
        
        # æŠ“å–è§†é¢‘æ•°æ®
        video_data = []
        try:
            # ç‚¹å‡»æ•°æ®èœå•
            await page.click('text=æ•°æ®')
            await page.wait_for_timeout(2000)
            
            # ç‚¹å‡»è§†é¢‘åˆ†æ
            await page.click('text=è§†é¢‘åˆ†æ')
            await page.wait_for_timeout(3000)
            
            # æå–è§†é¢‘åˆ—è¡¨æ•°æ®
            videos = await page.query_selector_all('.video-item')  # éœ€è¦æ ¹æ®å®é™…é¡µé¢è°ƒæ•´é€‰æ‹©å™¨
            
            for video in videos[:20]:  # æŠ“å–å‰20æ¡
                try:
                    title = await video.query_selector_eval('.video-title', 'el => el.textContent')
                    plays = await video.query_selector_eval('.play-count', 'el => el.textContent')
                    likes = await video.query_selector_eval('.like-count', 'el => el.textContent')
                    
                    video_data.append({
                        'title': title,
                        'plays': plays,
                        'likes': likes,
                        'scraped_at': datetime.now().isoformat()
                    })
                except:
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ æŠ–éŸ³æ•°æ®æŠ“å–éƒ¨åˆ†å¤±è´¥: {e}")
            
        self.results['douyin_videos'] = video_data
        return video_data
    
    async def scrape_meituan(self, page):
        """æŠ“å–ç¾å›¢å¼€åº—å®æ•°æ®"""
        print("ğŸœ æŠ“å–ç¾å›¢æ•°æ®...")
        
        await page.wait_for_load_state('networkidle')
        
        meituan_data = []
        try:
            # ç‚¹å‡»ç»è¥åˆ†æ
            await page.click('text=ç»è¥åˆ†æ')
            await page.wait_for_timeout(2000)
            
            # æŠ“å–äº¤æ˜“æ•°æ®
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…é¡µé¢ç»“æ„è°ƒæ•´
            
        except Exception as e:
            print(f"âš ï¸ ç¾å›¢æ•°æ®æŠ“å–éƒ¨åˆ†å¤±è´¥: {e}")
            
        self.results['meituan'] = meituan_data
        return meituan_data
    
    def save_results(self):
        """ä¿å­˜æŠ“å–ç»“æœ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ä¿å­˜ä¸ºJSON
        json_file = self.data_dir / f"scraped_data_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
            
        # ä¿å­˜ä¸ºCSVï¼ˆå¦‚æœæœ‰è§†é¢‘æ•°æ®ï¼‰
        if 'douyin_videos' in self.results:
            csv_file = self.data_dir / f"douyin_videos_{timestamp}.csv"
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                if self.results['douyin_videos']:
                    writer = csv.DictWriter(f, fieldnames=self.results['douyin_videos'][0].keys())
                    writer.writeheader()
                    writer.writerows(self.results['douyin_videos'])
                    
        print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {self.data_dir}")
        return json_file

async def main():
    scraper = AutoScraper()
    
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)  # å¯è§æ¨¡å¼ä¾¿äºè°ƒè¯•
        
        # æŠ“å–æŠ–éŸ³
        print("\\nğŸŒ æ‰“å¼€æŠ–éŸ³æ¥å®¢...")
        page = await browser.new_page()
        await page.goto('https://e.douyin.com/')
        print("â³ è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼Œç„¶åæŒ‰å›è½¦ç»§ç»­...")
        input()
        
        await scraper.scrape_douyin(page)
        
        # æŠ“å–ç¾å›¢
        print("\\nğŸŒ æ‰“å¼€ç¾å›¢å¼€åº—å®...")
        page2 = await browser.new_page()
        await page2.goto('https://e.waimai.meituan.com/')
        print("â³ è¯·åœ¨æµè§ˆå™¨ä¸­å®Œæˆç™»å½•ï¼Œç„¶åæŒ‰å›è½¦ç»§ç»­...")
        input()
        
        await scraper.scrape_meituan(page2)
        
        # ä¿å­˜ç»“æœ
        scraper.save_results()
        
        await browser.close()
        print("\\nâœ… æ•°æ®æŠ“å–å®Œæˆï¼")

if __name__ == '__main__':
    asyncio.run(main())
'''
        
        script_file = self.data_dir / "auto_scraper.py"
        with open(script_file, 'w', encoding='utf-8') as f:
            f.write(scraper_code)
            
        print(f"âœ… è‡ªåŠ¨åŒ–æŠ“å–è„šæœ¬å·²åˆ›å»º: {script_file}")
        return script_file
    
    def create_data_processor(self):
        """
        åˆ›å»ºæ•°æ®å¤„ç†å’Œåˆ†ææ¨¡å—
        """
        processor_code = '''#!/usr/bin/env python3
"""
è¿è¥æ•°æ®åˆ†æå¼•æ“
è‡ªåŠ¨åˆ†ææŠ“å–çš„æ•°æ®å¹¶ç”Ÿæˆè¿è¥å»ºè®®
"""

import json
import pandas as pd
from datetime import datetime
from pathlib import Path

class DataAnalyzer:
    def __init__(self):
        self.data_dir = Path.home() / ".openclaw/workspace/å¯†å®¤é€ƒè„±è¿è¥/æ•°æ®"
        self.analysis_dir = Path.home() / ".openclaw/workspace/å¯†å®¤é€ƒè„±è¿è¥/åˆ†æ"
        self.analysis_dir.mkdir(exist_ok=True)
        
    def load_data(self, data_file):
        """åŠ è½½æŠ“å–çš„æ•°æ®"""
        with open(data_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def analyze_douyin(self, data):
        """åˆ†ææŠ–éŸ³æ•°æ®"""
        print("ğŸ“Š åˆ†ææŠ–éŸ³æ•°æ®...")
        
        analysis = {
            'platform': 'æŠ–éŸ³æ¥å®¢',
            'analysis_time': datetime.now().isoformat(),
            'metrics': {}
        }
        
        if 'douyin_videos' in data:
            videos = data['douyin_videos']
            df = pd.DataFrame(videos)
            
            # è®¡ç®—å…³é”®æŒ‡æ ‡
            analysis['metrics'] = {
                'total_videos': len(videos),
                'avg_plays': df['plays'].mean() if 'plays' in df else 0,
                'avg_likes': df['likes'].mean() if 'likes' in df else 0,
                'engagement_rate': (df['likes'].sum() / df['plays'].sum() * 100) if 'plays' in df and 'likes' in df else 0
            }
            
            # æ‰¾å‡ºè¡¨ç°æœ€å¥½çš„è§†é¢‘
            if 'plays' in df:
                top_video = df.loc[df['plays'].idxmax()]
                analysis['top_performer'] = {
                    'title': top_video.get('title', ''),
                    'plays': top_video.get('plays', 0),
                    'likes': top_video.get('likes', 0)
                }
        
        return analysis
    
    def generate_insights(self, douyin_analysis, meituan_analysis):
        """ç”Ÿæˆè¿è¥æ´å¯Ÿå’Œå»ºè®®"""
        print("ğŸ’¡ ç”Ÿæˆè¿è¥æ´å¯Ÿ...")
        
        insights = {
            'generated_at': datetime.now().isoformat(),
            'summary': {},
            'recommendations': [],
            'action_items': []
        }
        
        # åŸºäºæ•°æ®çš„å»ºè®®
        if douyin_analysis.get('metrics', {}).get('engagement_rate', 0) < 5:
            insights['recommendations'].append({
                'priority': 'high',
                'area': 'å†…å®¹ä¼˜åŒ–',
                'suggestion': 'äº’åŠ¨ç‡åä½ï¼Œå»ºè®®å¢åŠ äº’åŠ¨å¼•å¯¼è¯æœ¯',
                'action': 'åœ¨è§†é¢‘ç»“å°¾æ·»åŠ "è¯„è®ºå‘Šè¯‰æˆ‘ä½ æœ€æƒ³ç©å“ªä¸ªä¸»é¢˜"'
            })
        
        # æ·»åŠ æ›´å¤šåŸºäºæ•°æ®çš„å»ºè®®...
        
        return insights
    
    def save_report(self, analysis, insights):
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.analysis_dir / f"è¿è¥åˆ†ææŠ¥å‘Š_{timestamp}.json"
        
        report = {
            'analysis': analysis,
            'insights': insights
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
            
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report_file

if __name__ == '__main__':
    analyzer = DataAnalyzer()
    # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„åˆ†ææµç¨‹
    print("æ•°æ®åˆ†æå¼•æ“å·²å‡†å¤‡å°±ç»ªï¼")
'''
        
        processor_file = self.data_dir / "data_analyzer.py"
        with open(processor_file, 'w', encoding='utf-8') as f:
            f.write(processor_code)
            
        print(f"âœ… æ•°æ®åˆ†æå¼•æ“å·²åˆ›å»º: {processor_file}")
        return processor_file
    
    def create_content_generator(self):
        """
        åˆ›å»ºçˆ†æ¬¾å†…å®¹ç”Ÿæˆå™¨
        """
        generator_code = '''#!/usr/bin/env python3
"""
çˆ†æ¬¾å†…å®¹ç”Ÿæˆå™¨
åŸºäºæ•°æ®åˆ†æå’Œè·¨è¡Œä¸šçµæ„Ÿç”Ÿæˆè§†é¢‘æ–‡æ¡ˆ
"""

import json
import random
from datetime import datetime
from pathlib import Path

class ContentGenerator:
    def __init__(self):
        self.content_dir = Path.home() / ".openclaw/workspace/å¯†å®¤é€ƒè„±è¿è¥/å†…å®¹"
        self.content_dir.mkdir(exist_ok=True)
        
        # çˆ†æ¬¾æ ‡é¢˜æ¨¡æ¿
        self.title_templates = [
            "âš ï¸ èƒ†å°æ…å…¥ï¼è¿™å®¶å¯†å®¤è®©æˆ‘{emotion}",
            "ğŸ”¥ å…¨ç½‘æœ€ç«çš„å¯†å®¤ä¸»é¢˜ï¼Œ{achievement}ï¼",
            "ğŸ˜± 99%çš„äººéƒ½é€ƒä¸å‡ºå»çš„å¯†å®¤ï¼Œ{challenge}",
            "ğŸ’¡ å¯†å®¤é€ƒè„±å¿…çŸ¥çš„{number}ä¸ªæŠ€å·§",
            "ğŸ† æˆ‘ä»¬ä»…ç”¨{time}å°±é€ƒå‡ºæ¥äº†ï¼",
            "ğŸ’° äººå‡{price}å…ƒï¼Œä½“éªŒ{feature}ï¼",
            "ğŸ­ ç©å®Œè¿™ä¸ªå¯†å®¤ï¼Œæˆ‘{reaction}...",
            "ğŸ‘» å’Œ{relation}ç©ææ€–å¯†å®¤ï¼Œç»“æœ...",
            "ğŸ¤¯ {subject}è¢«ç¢¾å‹çš„ä¸€å¤©",
            "ğŸ†š {comparison}ç©å¯†å®¤çš„åŒºåˆ«"
        ]
        
        # æƒ…ç»ªè¯åº“
        self.emotions = ["å“å“­äº†", "å°–å«ä¸æ­¢", "å¤œä¸èƒ½å¯", "å›å‘³æ— ç©·", "æ¬²ç½¢ä¸èƒ½"]
        self.achievements = ["ç»ˆäºæ‰“å¡äº†", "æ’äº†3å‘¨é˜Ÿ", "äºŒåˆ·éƒ½ä¸å¤Ÿ", "è¦å¸¦æ‰€æœ‰äººæ¥"]
        self.challenges = ["ä½ æ•¢æŒ‘æˆ˜å—", "ä½ èƒ½ç ´çºªå½•å—", "ä½ èƒ½ä¿æŒå†·é™å—"]
        
    def generate_video_script(self, theme="ææ€–", difficulty="ä¸­ç­‰", players="4-6äºº"):
        """ç”Ÿæˆè§†é¢‘è„šæœ¬"""
        
        # ç”Ÿæˆæ ‡é¢˜
        title = self._generate_title(theme)
        
        # è„šæœ¬ç»“æ„
        script = {
            'title': title,
            'theme': theme,
            'difficulty': difficulty,
            'players': players,
            'duration': '60-90ç§’',
            'structure': {
                'hook': self._generate_hook(theme),
                'setup': self._generate_setup(theme),
                'climax': self._generate_climax(theme),
                'cta': self._generate_cta()
            },
            'hashtags': self._generate_hashtags(theme),
            'bgm_suggestions': self._generate_bgm(theme),
            'generated_at': datetime.now().isoformat()
        }
        
        return script
    
    def _generate_title(self, theme):
        """ç”Ÿæˆçˆ†æ¬¾æ ‡é¢˜"""
        template = random.choice(self.title_templates)
        
        replacements = {
            'emotion': random.choice(self.emotions),
            'achievement': random.choice(self.achievements),
            'challenge': random.choice(self.challenges),
            'number': random.choice(['3', '5', '7']),
            'time': random.choice(['30åˆ†é’Ÿ', '45åˆ†é’Ÿ', '1å°æ—¶']),
            'price': random.choice(['68', '88', '99', '128']),
            'feature': random.choice(['ç”µå½±çº§åœºæ™¯', 'æ²‰æµ¸å¼ä½“éªŒ', 'çƒ§è„‘è§£è°œ']),
            'reaction': random.choice(['å“­äº†', 'ç¬‘äº†', 'æƒŠå‘†äº†']),
            'relation': random.choice(['æš—æ‹å¯¹è±¡', 'é—ºèœœ', 'å…„å¼Ÿ', 'å¯¹è±¡']),
            'subject': random.choice(['æ™ºå•†', 'èƒ†é‡', 'ä½“åŠ›']),
            'comparison': random.choice(['æ–°æ‰‹vsé«˜æ‰‹', 'ç”·ç”Ÿvså¥³ç”Ÿ', 'ç¤¾ç‰›vsç¤¾æ'])
        }
        
        title = template
        for key, value in replacements.items():
            title = title.replace('{' + key + '}', value)
            
        return title
    
    def _generate_hook(self, theme):
        """ç”Ÿæˆé»„é‡‘3ç§’é’©å­"""
        hooks = {
            'ææ€–': [
                "è¿™ä¸ªå¯†å®¤ï¼Œè®©æˆ‘æ•´æ•´ä¸€å‘¨ä¸æ•¢å…³ç¯ç¡è§‰...",
                "èƒ†å°å‹¿å…¥ï¼è¿™ä¸ªä¸»é¢˜çš„NPCä¼šè¿½ç€ä½ åœ¨å…¨é¦†è·‘...",
                "æˆ‘æ•¢æ‰“èµŒï¼Œä½ ç»å¯¹æ’‘ä¸è¿‡å‰10åˆ†é’Ÿï¼"
            ],
            'æ‚¬ç–‘': [
                "è¿™ä¸ªå¯†å®¤çš„å‰§æƒ…ï¼Œæ¯”ç”µå½±è¿˜ç²¾å½©ï¼",
                "æˆ‘ä»¬è§£åˆ°æœ€åæ‰å‘ç°ï¼ŒçœŸç›¸ç«Ÿç„¶æ˜¯...",
                "99%çš„äººçŒœä¸åˆ°ç»“å±€çš„å¯†å®¤ä¸»é¢˜ï¼"
            ],
            'è§£è°œ': [
                "æ™ºå•†140ä»¥ä¸‹åˆ«æ¥æŒ‘æˆ˜è¿™ä¸ªå¯†å®¤ï¼",
                "è¿™ä¸ªå¯†å®¤çš„è°œé¢˜ï¼Œæˆ‘ä»¬å›¢é˜Ÿè®¨è®ºäº†æ•´æ•´ä¸€å‘¨...",
                "å·ç§°æœ€éš¾å¯†å®¤ï¼Œæˆ‘ä»¬èƒ½å¦ç ´çºªå½•ï¼Ÿ"
            ]
        }
        
        return random.choice(hooks.get(theme, hooks['ææ€–']))
    
    def _generate_setup(self, theme):
        """ç”Ÿæˆåœºæ™¯é“ºå«"""
        return "åœºæ™¯æè¿°å’Œæ°›å›´è¥é€ ..."
    
    def _generate_climax(self, theme):
        """ç”Ÿæˆé«˜æ½®éƒ¨åˆ†"""
        return "æœ€ç²¾å½©çš„æ¸¸æˆç‰‡æ®µå’Œç©å®¶ååº”..."
    
    def _generate_cta(self):
        """ç”Ÿæˆè¡ŒåŠ¨å·å¬"""
        ctas = [
            "ç‚¹å‡»å·¦ä¸‹è§’å›¢è´­ï¼Œé™æ—¶ä¼˜æƒ ä¸­ï¼",
            "è¯„è®ºåŒºå‘Šè¯‰æˆ‘ä½ æœ€æƒ³æŒ‘æˆ˜å“ªä¸ªä¸»é¢˜ï¼",
            "å…³æ³¨+ç‚¹èµï¼Œä¸‹æœŸå¸¦ä½ è§£é”éšè—ç»“å±€ï¼",
            "å¿«å¸¦ä¸Šä½ çš„å†¤ç§æœ‹å‹æ¥æŒ‘æˆ˜ï¼"
        ]
        return random.choice(ctas)
    
    def _generate_hashtags(self, theme):
        """ç”Ÿæˆæ ‡ç­¾"""
        base_tags = ["#å¯†å®¤é€ƒè„±", "#å¯†å®¤", "#å‘¨æœ«å»å“ªå„¿", "#é•¿æ²™å¯†å®¤"]
        theme_tags = {
            'ææ€–': ["#ææ€–å¯†å®¤", "#èƒ†å°æ…å…¥", "#åˆºæ¿€ä½“éªŒ"],
            'æ‚¬ç–‘': ["#æ‚¬ç–‘æ¨ç†", "#çƒ§è„‘", "#å‰§æœ¬æ€"],
            'è§£è°œ': ["#è§£è°œæ¸¸æˆ", "#æ™ºå•†æŒ‘æˆ˜", "#æ¨ç†"]
        }
        return base_tags + theme_tags.get(theme, [])
    
    def _generate_bgm(self, theme):
        """ç”ŸæˆBGMå»ºè®®"""
        bgms = {
            'ææ€–': ["æ‚¬ç–‘ç´§å¼ BGM", "ææ€–æ°›å›´éŸ³ä¹", "ã€Šå°ç™½èˆ¹ã€‹å˜å¥ç‰ˆ"],
            'æ‚¬ç–‘': ["æ¨ç†ä¾¦æ¢BGM", "ç´§å¼ èŠ‚å¥éŸ³ä¹", "ã€Šåä¾¦æ¢æŸ¯å—ã€‹BGM"],
            'è§£è°œ': ["è½»å¿«æ™ºåŠ›BGM", "æŒ‘æˆ˜èŠ‚å¥éŸ³ä¹", "æ¸¸æˆé—¯å…³BGM"]
        }
        return bgms.get(theme, ["çƒ­é—¨BGM"])
    
    def batch_generate(self, count=5):
        """æ‰¹é‡ç”Ÿæˆå†…å®¹"""
        contents = []
        themes = ['ææ€–', 'æ‚¬ç–‘', 'è§£è°œ']
        
        for i in range(count):
            theme = random.choice(themes)
            script = self.generate_video_script(theme)
            contents.append(script)
            
        # ä¿å­˜ç”Ÿæˆçš„å†…å®¹
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = self.content_dir / f"ç”Ÿæˆçš„è„šæœ¬_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(contents, f, ensure_ascii=False, indent=2)
            
        print(f"âœ… å·²ç”Ÿæˆ{count}ä¸ªè§†é¢‘è„šæœ¬: {output_file}")
        return contents

if __name__ == '__main__':
    generator = ContentGenerator()
    generator.batch_generate(5)
'''
        
        generator_file = self.data_dir / "content_generator.py"
        with open(generator_file, 'w', encoding='utf-8') as f:
            f.write(generator_code)
            
        print(f"âœ… çˆ†æ¬¾å†…å®¹ç”Ÿæˆå™¨å·²åˆ›å»º: {generator_file}")
        return generator_file


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ å¯†å®¤é€ƒè„±è¿è¥æ•°æ®æŠ“å–ç³»ç»Ÿ")
    print("=" * 60)
    
    scraper = DataScraper()
    
    # åˆ›å»ºæ‰€æœ‰å¿…è¦çš„ç»„ä»¶
    print("\\nğŸ“¦ æ­£åœ¨åˆ›å»ºæ•°æ®æŠ“å–ç³»ç»Ÿ...")
    
    # 1. åˆ›å»ºå¹³å°é…ç½®
    douyin_config = scraper.scrape_douyin_data()
    meituan_config = scraper.scrape_meituan_data()
    
    # 2. åˆ›å»ºè‡ªåŠ¨åŒ–æŠ“å–è„šæœ¬
    scraper_script = scraper.create_scraper_script()
    
    # 3. åˆ›å»ºæ•°æ®åˆ†æå¼•æ“
    analyzer_script = scraper.create_data_processor()
    
    # 4. åˆ›å»ºå†…å®¹ç”Ÿæˆå™¨
    generator_script = scraper.create_content_generator()
    
    print("\\n" + "=" * 60)
    print("âœ… ç³»ç»Ÿéƒ¨ç½²å®Œæˆï¼")
    print("=" * 60)
    print("\\nğŸ“‹ ä½¿ç”¨æ­¥éª¤ï¼š")
    print("1. å®‰è£…ä¾èµ–: pip install playwright pandas")
    print("2. å®‰è£…æµè§ˆå™¨: playwright install")
    print("3. è¿è¡ŒæŠ“å–: python3", scraper_script.name)
    print("4. åˆ†ææ•°æ®: python3", analyzer_script.name)
    print("5. ç”Ÿæˆå†…å®¹: python3", generator_script.name)
    print("\\nğŸ“ æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨:", scraper.data_dir)
    print("=" * 60)


if __name__ == '__main__':
    main()
