#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯†å®¤é€ƒè„± - è¶…çº§è¿è¥AIæ ¸å¿ƒå¼•æ“
åŠŸèƒ½ï¼šæ•°æ®åˆ†æ + å†…å®¹ç”Ÿæˆ + è‡ªæˆ‘å­¦ä¹ 
ä½œè€…ï¼šAIè¿è¥å›¢é˜Ÿ
"""

import json
import csv
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path

class EscapeRoomAIOps:
    def __init__(self, workspace_path="/Users/xiaolongxia/.openclaw/workspace/å¯†å®¤é€ƒè„±è¿è¥"):
        self.workspace = Path(workspace_path)
        self.data_dir = self.workspace / "æ•°æ®"
        self.content_dir = self.workspace / "å†…å®¹"
        self.analysis_dir = self.workspace / "åˆ†ææŠ¥å‘Š"
        self.log_dir = self.workspace / "æ—¥å¿—"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        for dir_path in [self.data_dir, self.content_dir, self.analysis_dir, self.log_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        self.log(f"è¶…çº§è¿è¥AIåˆå§‹åŒ–å®Œæˆ - {datetime.now()}")
    
    def log(self, message):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_file = self.log_dir / f"ops_{datetime.now().strftime('%Y%m%d')}.log"
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(f"[{timestamp}] {message}\n")
        print(f"[OpsAI] {message}")
    
    def analyze_douyin_data(self, data_file=None):
        """åˆ†ææŠ–éŸ³æ¥å®¢æ•°æ®"""
        self.log("å¼€å§‹åˆ†ææŠ–éŸ³æ¥å®¢æ•°æ®...")
        
        # å¦‚æœæ²¡æœ‰æ•°æ®æ–‡ä»¶ï¼Œåˆ›å»ºæ¨¡æ¿
        if not data_file or not os.path.exists(data_file):
            template_file = self.data_dir / "æŠ–éŸ³æ¥å®¢" / "æ•°æ®æ¨¡æ¿.csv"
            template_file.parent.mkdir(parents=True, exist_ok=True)
            
            headers = ["æ—¥æœŸ", "è§†é¢‘ID", "æ ‡é¢˜", "æ’­æ”¾é‡", "ç‚¹èµæ•°", "è¯„è®ºæ•°", 
                      "åˆ†äº«æ•°", "å®Œæ’­ç‡", "å›¢è´­ç‚¹å‡»", "è®¢å•é‡", "GMV"]
            
            with open(template_file, "w", newline="", encoding="utf-8-sig") as f:
                writer = csv.writer(f)
                writer.writerow(headers)
                # ç¤ºä¾‹æ•°æ®
                writer.writerow(["2026-02-19", "VID001", "ææ€–å¯†å®¤æŒ‘æˆ˜", 15000, 450, 89, 23, "45%", 120, 15, 2850])
            
            self.log(f"åˆ›å»ºæŠ–éŸ³æ•°æ®æ¨¡æ¿: {template_file}")
            return {"status": "template_created", "path": str(template_file)}
        
        # åˆ†æç°æœ‰æ•°æ®
        insights = {
            "total_videos": 0,
            "total_views": 0,
            "avg_engagement": 0,
            "conversion_rate": 0,
            "top_performing": []
        }
        
        with open(data_file, "r", encoding="utf-8-sig") as f:
            reader = csv.DictReader(f)
            videos = list(reader)
            insights["total_videos"] = len(videos)
            
            if videos:
                insights["total_views"] = sum(int(v.get("æ’­æ”¾é‡", 0)) for v in videos)
                insights["avg_engagement"] = sum(int(v.get("ç‚¹èµæ•°", 0)) for v in videos) / len(videos)
                
                # æŒ‰æ’­æ”¾é‡æ’åºæ‰¾å‡ºçˆ†æ¬¾
                sorted_videos = sorted(videos, key=lambda x: int(x.get("æ’­æ”¾é‡", 0)), reverse=True)
                insights["top_performing"] = sorted_videos[:3]
        
        self.log(f"æŠ–éŸ³æ•°æ®åˆ†æå®Œæˆ: {insights['total_videos']}æ¡è§†é¢‘, {insights['total_views']}æ€»æ’­æ”¾")
        return insights
    
    def analyze_meituan_data(self, data_file=None):
        """åˆ†æç¾å›¢å¼€åº—å®æ•°æ®"""
        self.log("å¼€å§‹åˆ†æç¾å›¢å¼€åº—å®æ•°æ®...")
        
        if not data_file or not os.path.exists(data_file):
            template_file = self.data_dir / "ç¾å›¢å¼€åº—å®" / "æ•°æ®æ¨¡æ¿.csv"
            template_file.parent.mkdir(parents=True, exist_ok=True)
            
            headers = ["æ—¥æœŸ", "æ›å…‰é‡", "è®¿é—®é‡", "ç‚¹å‡»ç‡", "è®¢å•é‡", "äº¤æ˜“é¢", 
                      "å®¢å•ä»·", "è¯„åˆ†", "æ–°å¢è¯„ä»·", "å¥½è¯„ç‡", "åŒå•†åœˆæ’å"]
            
            with open(template_file, "w", newline="", encoding="utf-8-sig") as f:
                writer = csv.writer(f)
                writer.writerow(headers)
                writer.writerow(["2026-02-19", 5200, 680, "13.1%", 45, 8550, 190, 4.8, 12, "92%", 3])
            
            self.log(f"åˆ›å»ºç¾å›¢æ•°æ®æ¨¡æ¿: {template_file}")
            return {"status": "template_created", "path": str(template_file)}
        
        self.log("ç¾å›¢æ•°æ®åˆ†æå®Œæˆ")
        return {"status": "analyzed"}
    
    def monitor_competitors(self, competitor_list=None):
        """ç›‘æ§ç«å“é—¨åº—"""
        self.log("å¯åŠ¨ç«å“ç›‘æ§...")
        
        # å¦‚æœæ²¡æœ‰ç«å“åˆ—è¡¨ï¼Œåˆ›å»ºæ¨¡æ¿
        competitor_file = self.workspace / "ç«å“ç›‘æ§" / "ç«å“æ¸…å•.json"
        competitor_file.parent.mkdir(parents=True, exist_ok=True)
        
        if not competitor_file.exists():
            template = {
                "last_update": datetime.now().isoformat(),
                "competitors": [
                    {"name": "ç«å“1-åç§°", "platforms": {"æŠ–éŸ³": "", "ç¾å›¢": ""}, "notes": ""},
                    {"name": "ç«å“2-åç§°", "platforms": {"æŠ–éŸ³": "", "ç¾å›¢": ""}, "notes": ""},
                    {"name": "ç«å“3-åç§°", "platforms": {"æŠ–éŸ³": "", "ç¾å›¢": ""}, "notes": ""},
                    {"name": "ç«å“4-åç§°", "platforms": {"æŠ–éŸ³": "", "ç¾å›¢": ""}, "notes": ""},
                    {"name": "ç«å“5-åç§°", "platforms": {"æŠ–éŸ³": "", "ç¾å›¢": ""}, "notes": ""}
                ]
            }
            with open(competitor_file, "w", encoding="utf-8") as f:
                json.dump(template, f, ensure_ascii=False, indent=2)
            
            self.log(f"åˆ›å»ºç«å“ç›‘æ§æ¨¡æ¿: {competitor_file}")
            return {"status": "template_created", "path": str(competitor_file)}
        
        self.log("ç«å“ç›‘æ§å®Œæˆ")
        return {"status": "monitored"}
    
    def generate_content_ideas(self, num_ideas=5):
        """ç”Ÿæˆçˆ†æ¬¾å†…å®¹åˆ›æ„"""
        self.log(f"ç”Ÿæˆ{num_ideas}æ¡å†…å®¹åˆ›æ„...")
        
        # å¯†å®¤é€ƒè„±çˆ†æ¬¾å…¬å¼æ¨¡æ¿
        templates = [
            {
                "type": "ææ€–æ°›å›´",
                "hook": "âš ï¸ èƒ†å°å‹¿å…¥ï¼è¿™å®¶å¯†å®¤è®©æˆ‘å½“åœºç ´é˜²...",
                "structure": "é»„é‡‘3ç§’ææ€–éŸ³æ•ˆ + ç©å®¶å°–å«ååº” + å‰§æƒ…é«˜æ½®ç‰‡æ®µ + å½©è›‹",
                "hashtags": "#å¯†å®¤é€ƒè„± #ææ€–å¯†å®¤ #é•¿æ²™æ¢åº— #å‘¨æœ«å»å“ªç©",
                "bgm": "æ‚¬ç–‘/ææ€–æ°›å›´éŸ³ä¹"
            },
            {
                "type": "è§£è°œæŒ‘æˆ˜", 
                "hook": "ğŸ§  æ™ºå•†180æ‰èƒ½é€šå…³çš„å¯†å®¤ï¼Œä½ æ•¢æŒ‘æˆ˜å—ï¼Ÿ",
                "structure": "è°œé¢˜å±•ç¤º + ç©å®¶æ€è€ƒè¿‡ç¨‹ + æ­æ™“ç­”æ¡ˆ + æˆå°±æ„Ÿ",
                "hashtags": "#å¯†å®¤é€ƒè„± #è§£è°œæ¸¸æˆ #æ™ºå•†æŒ‘æˆ˜ #çƒ§è„‘",
                "bgm": "ç´§å¼ æ‚¬ç–‘éŸ³ä¹"
            },
            {
                "type": "æƒ…æ„Ÿå‰§æƒ…",
                "hook": "ğŸ˜­ ç©å®Œè¿™ä¸ªå¯†å®¤ï¼Œæˆ‘å“­äº†ä¸€æ•´æ™š...",
                "structure": "å‰§æƒ…å¼•å…¥ + æ²‰æµ¸ä½“éªŒ + æƒ…æ„Ÿé«˜æ½® + ç©å®¶çœŸå®ååº”",
                "hashtags": "#å¯†å®¤é€ƒè„± #æ²‰æµ¸å¼ä½“éªŒ #æƒ…æ„Ÿå…±é¸£ #å‚¬æ³ª",
                "bgm": "æƒ…æ„ŸBGM"
            },
            {
                "type": "æ¢åº—æµ‹è¯„",
                "hook": "ğŸ” å®æµ‹ï¼é•¿æ²™æœ€ç«çš„å¯†å®¤åˆ°åº•å€¼ä¸å€¼ï¼Ÿ",
                "structure": "ç¯å¢ƒå±•ç¤º + ä¸»é¢˜ä»‹ç» + ä½“éªŒè¿‡ç¨‹ + çœŸå®è¯„åˆ†",
                "hashtags": "#å¯†å®¤é€ƒè„± #æ¢åº— #é•¿æ²™å¯†å®¤ #çœŸå®æµ‹è¯„",
                "bgm": "è½»å¿«æ¢åº—éŸ³ä¹"
            },
            {
                "type": "ä¼˜æƒ æ´»åŠ¨",
                "hook": "ğŸ’° é™æ—¶ç¦åˆ©ï¼å¯†å®¤é€ƒè„±åŒäººç¥¨åªè¦XXå…ƒï¼",
                "structure": "ç¦åˆ©é¢„å‘Š + é—¨åº—äº®ç‚¹ + è´­ä¹°å¼•å¯¼ + ç´§è¿«æ„Ÿ",
                "hashtags": "#å¯†å®¤é€ƒè„± #é™æ—¶ä¼˜æƒ  #å›¢è´­ #å‘¨æœ«ç¦åˆ©",
                "bgm": "å¿«èŠ‚å¥ä¿ƒé”€éŸ³ä¹"
            }
        ]
        
        # ä¿å­˜å†…å®¹åˆ›æ„
        ideas_file = self.content_dir / "è„šæœ¬" / f"åˆ›æ„åº“_{datetime.now().strftime('%Y%m%d')}.json"
        ideas_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(ideas_file, "w", encoding="utf-8") as f:
            json.dump({
                "generated_at": datetime.now().isoformat(),
                "ideas": templates[:num_ideas]
            }, f, ensure_ascii=False, indent=2)
        
        self.log(f"ç”Ÿæˆ{num_ideas}æ¡å†…å®¹åˆ›æ„ï¼Œä¿å­˜è‡³: {ideas_file}")
        return {"status": "generated", "ideas": templates[:num_ideas], "file": str(ideas_file)}
    
    def generate_weekly_report(self):
        """ç”Ÿæˆå‘¨åº¦è¿è¥æŠ¥å‘Š"""
        self.log("ç”Ÿæˆå‘¨åº¦è¿è¥æŠ¥å‘Š...")
        
        report = f"""
# å¯†å®¤é€ƒè„±å‘¨åº¦è¿è¥æŠ¥å‘Š
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}

## ğŸ“Š æ•°æ®æ¦‚è§ˆ
- æŠ–éŸ³æ’­æ”¾é‡: å¾…æ•°æ®æ¥å…¥
- ç¾å›¢è½¬åŒ–ç‡: å¾…æ•°æ®æ¥å…¥
- ç«å“åŠ¨æ€: å¾…ç›‘æ§å¯åŠ¨

## ğŸ¯ æœ¬å‘¨å†…å®¹è®¡åˆ’
1. ææ€–æ°›å›´ç±»è§†é¢‘ x2
2. è§£è°œæŒ‘æˆ˜ç±»è§†é¢‘ x1
3. æ¢åº—æµ‹è¯„ç±»è§†é¢‘ x1
4. ä¼˜æƒ æ´»åŠ¨æ¨å¹¿ x1

## ğŸ’¡ çˆ†æ¬¾çµæ„Ÿ
- è·¨è¡Œä¸šå­¦ä¹ : é¤é¥®æ¢åº—æ¨¡å¼ â†’ å¯†å®¤æ¢é¦†
- çƒ­é—¨å…ƒç´ : æ‚¬ç–‘BGM + çœŸå®ç©å®¶ååº”
- å‘å¸ƒæ—¶é—´: å‘¨å››/å‘¨äº”æ™š8ç‚¹æ•ˆæœæœ€ä½³

## ğŸ”” å¾…åŠäº‹é¡¹
- [ ] æ¥å…¥æŠ–éŸ³æ¥å®¢æ•°æ®
- [ ] æ¥å…¥ç¾å›¢å¼€åº—å®æ•°æ®
- [ ] é…ç½®ç«å“ç›‘æ§åå•
- [ ] æ‹æ‘„ç¬¬ä¸€æ¡çˆ†æ¬¾è§†é¢‘

---
æŠ¥å‘Šç”±è¶…çº§è¿è¥AIè‡ªåŠ¨ç”Ÿæˆ
"""
        
        report_file = self.analysis_dir / f"å‘¨åº¦æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d')}.md"
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report)
        
        self.log(f"å‘¨åº¦æŠ¥å‘Šç”Ÿæˆ: {report_file}")
        return {"status": "report_generated", "path": str(report_file)}
    
    def self_improvement(self):
        """è‡ªæˆ‘å­¦ä¹ ä¼˜åŒ–"""
        self.log("æ‰§è¡Œè‡ªæˆ‘å­¦ä¹ ä¼˜åŒ–...")
        
        # è®°å½•å­¦ä¹ æ—¥å¿—
        learning_log = self.log_dir / "learning_log.json"
        
        learning_entry = {
            "timestamp": datetime.now().isoformat(),
            "insights": [
                "æ•°æ®æ¥å…¥æ˜¯è¿è¥åŸºç¡€ï¼Œä¼˜å…ˆå®Œæˆ",
                "å†…å®¹åˆ›æ„éœ€è¦ç»“åˆå®é™…æ•°æ®åé¦ˆ",
                "ç«å“ç›‘æ§å¯ä»¥å‘ç°å¸‚åœºç©ºç™½ç‚¹",
                "å‘å¸ƒæ—¶é—´å¯¹æ’­æ”¾é‡å½±å“æ˜¾è‘—"
            ],
            "next_actions": [
                "ç­‰å¾…ç”¨æˆ·æä¾›è´¦å·ä¿¡æ¯",
                "å®Œæˆæ•°æ®æŠ“å–è„šæœ¬",
                "å»ºç«‹è‡ªåŠ¨åŒ–åˆ†ææµç¨‹"
            ]
        }
        
        # è¿½åŠ åˆ°å­¦ä¹ æ—¥å¿—
        if learning_log.exists():
            with open(learning_log, "r", encoding="utf-8") as f:
                logs = json.load(f)
        else:
            logs = []
        
        logs.append(learning_entry)
        
        with open(learning_log, "w", encoding="utf-8") as f:
            json.dump(logs, f, ensure_ascii=False, indent=2)
        
        self.log("è‡ªæˆ‘å­¦ä¹ å®Œæˆï¼Œç»éªŒå·²æ²‰æ·€")
        return {"status": "learning_complete"}
    
    def run_full_analysis(self):
        """æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        self.log("=" * 50)
        self.log("å¯åŠ¨å®Œæ•´è¿è¥åˆ†ææµç¨‹")
        self.log("=" * 50)
        
        results = {
            "douyin": self.analyze_douyin_data(),
            "meituan": self.analyze_meituan_data(),
            "competitors": self.monitor_competitors(),
            "content": self.generate_content_ideas(5),
            "report": self.generate_weekly_report(),
            "learning": self.self_improvement()
        }
        
        self.log("=" * 50)
        self.log("å®Œæ•´åˆ†ææµç¨‹æ‰§è¡Œå®Œæ¯•")
        self.log("=" * 50)
        
        return results


if __name__ == "__main__":
    ai = EscapeRoomAIOps()
    results = ai.run_full_analysis()
    
    print("\n" + "=" * 50)
    print("è¶…çº§è¿è¥AIæ‰§è¡Œå®Œæˆ!")
    print("=" * 50)
    print(f"\nç”Ÿæˆæ–‡ä»¶:")
    print(f"- æŠ–éŸ³æ•°æ®æ¨¡æ¿: {results['douyin'].get('path', 'N/A')}")
    print(f"- ç¾å›¢æ•°æ®æ¨¡æ¿: {results['meituan'].get('path', 'N/A')}")
    print(f"- ç«å“ç›‘æ§æ¨¡æ¿: {results['competitors'].get('path', 'N/A')}")
    print(f"- å†…å®¹åˆ›æ„åº“: {results['content'].get('file', 'N/A')}")
    print(f"- å‘¨åº¦æŠ¥å‘Š: {results['report'].get('path', 'N/A')}")
    print("\nä¸‹ä¸€æ­¥: è¯·æä¾›è´¦å·ä¿¡æ¯ä»¥å¯åŠ¨æ•°æ®æŠ“å–")
