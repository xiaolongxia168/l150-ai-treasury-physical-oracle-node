#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯†å®¤é€ƒè„±è¿è¥AIçŸ¥è¯†åº“åŠ è½½å™¨
å°†è½¬å½•çš„è¯¾ç¨‹å†…å®¹æ³¨å…¥AIæ•°å­—è¿è¥çš„çŸ¥è¯†åº“
"""

import os
import json
from pathlib import Path
from datetime import datetime

# çŸ¥è¯†åº“è·¯å¾„
KNOWLEDGE_BASE_DIR = Path("/Users/xiaolongxia/.openclaw/workspace/å¯†å®¤é€ƒè„±è¿è¥/çŸ¥è¯†åº“/è¯¾ç¨‹è½¬å½•")
OUTPUT_DIR = Path("/Users/xiaolongxia/.openclaw/workspace/å¯†å®¤é€ƒè„±è¿è¥/çŸ¥è¯†åº“")

def load_all_transcripts():
    """åŠ è½½æ‰€æœ‰è½¬å½•æ–‡ä»¶"""
    if not KNOWLEDGE_BASE_DIR.exists():
        print(f"âŒ çŸ¥è¯†åº“ç›®å½•ä¸å­˜åœ¨: {KNOWLEDGE_BASE_DIR}")
        return []
    
    transcripts = []
    for json_file in sorted(KNOWLEDGE_BASE_DIR.glob("*.json")):
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            transcripts.append(data)
    
    return transcripts

def generate_knowledge_summary():
    """ç”ŸæˆçŸ¥è¯†åº“æ‘˜è¦æŠ¥å‘Š"""
    transcripts = load_all_transcripts()
    
    if not transcripts:
        print("âš ï¸  æš‚æ— è½¬å½•å†…å®¹")
        return
    
    print(f"ğŸ“š å…±åŠ è½½ {len(transcripts)} ä¸ªè¯¾ç¨‹è½¬å½•")
    
    # æŒ‰åˆ†ç±»ç»Ÿè®¡
    categories = {}
    for t in transcripts:
        cat = t.get("category", "å…¶ä»–")
        if cat not in categories:
            categories[cat] = []
        categories[cat].append(t)
    
    print("\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
    for cat, items in sorted(categories.items()):
        print(f"  â€¢ {cat}: {len(items)} ä¸ª")
    
    # ç”Ÿæˆæ€»çŸ¥è¯†åº“æ–‡æ¡£
    summary_file = OUTPUT_DIR / "ç¾å›¢è¿è¥è¯¾ç¨‹çŸ¥è¯†åº“.md"
    
    content = f"""# ç¾å›¢è¿è¥è¯¾ç¨‹çŸ¥è¯†åº“

> è‡ªåŠ¨ç”Ÿæˆäº: {datetime.now().strftime('%Y-%m-%d %H:%M')}
> æ¥æº: å·…å³°æµé‡Â·å®ä½“å›¢è´­æ“ç›˜æ‰‹è¯¾ç¨‹
> å…± {len(transcripts)} ä¸ªè¯¾ç¨‹è½¬å½•

## ç›®å½•

"""
    
    # æŒ‰åˆ†ç±»ç»„ç»‡
    for cat, items in sorted(categories.items()):
        content += f"\n### {cat}\n\n"
        for item in items:
            content += f"- **{item['video_name']}**\n"
            content += f"  - å…³é”®è¯: {', '.join(item.get('keywords', []))}\n"
            content += f"  - æ‘˜è¦: {item.get('summary', '')[:100]}...\n\n"
    
    # æ·»åŠ æ£€ç´¢æç¤º
    content += """
## AIè¿è¥åŠ©æ‰‹ä½¿ç”¨æç¤º

å½“å›ç­”ä»¥ä¸‹é—®é¢˜æ—¶ï¼Œå‚è€ƒæœ¬çŸ¥è¯†åº“å†…å®¹ï¼š

### è¯„ä»·ç®¡ç†
- å¦‚ä½•æå‡åº—é“ºæ˜Ÿçº§è¯„åˆ†ï¼Ÿ
- å¦‚ä½•å¤„ç†å·®è¯„ï¼Ÿ
- ABè´¦å·ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ
- åŒè¯„æ³•å’Œæ ¸è¯„æ¯”å¦‚ä½•æ“ä½œï¼Ÿ

### æ¨å¹¿æŠ•æ”¾
- æ¨å¹¿é€šå¦‚ä½•è®¾ç½®å‡ºä»·ï¼Ÿ
- é€šæŠ•æ‹‰æ»¡ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ
- å¦‚ä½•ç”¨å¾®ä»˜è´¹æ’¬åŠ¨è‡ªç„¶æµé‡ï¼Ÿ

### æ•°æ®åˆ†æ
- åå°ä¸‰å¤§æ ¸å¿ƒæ•°æ®æŒ‡æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ
- å¦‚ä½•åˆ†ææµé‡æ¥æºï¼Ÿ
- è½¬åŒ–ç‡ä¼˜åŒ–æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ

### æ¦œå•è¿è¥
- å¦‚ä½•å†²å‡»çƒ­é—¨æ¦œå•ï¼Ÿ
- æ¦œå•æ’åçš„å½±å“å› ç´ ï¼Ÿ
- å¦‚ä½•åˆ©ç”¨æ¦œå•å¸¦æ¥æ›´å¤šæµé‡ï¼Ÿ

---
*æœ¬çŸ¥è¯†åº“ç”±è§†é¢‘è½¬å½•è‡ªåŠ¨ç”Ÿæˆï¼Œä¾›AIæ•°å­—è¿è¥ç³»ç»Ÿä½¿ç”¨*
"""
    
    summary_file.write_text(content, encoding="utf-8")
    print(f"\nâœ… çŸ¥è¯†åº“æ‘˜è¦å·²ç”Ÿæˆ: {summary_file}")
    
    # ç”ŸæˆRAGå‹å¥½çš„JSONæ ¼å¼
    rag_file = OUTPUT_DIR / "knowledge_base_rag.json"
    rag_data = {
        "version": "1.0",
        "generated_at": datetime.now().isoformat(),
        "total_courses": len(transcripts),
        "categories": {cat: len(items) for cat, items in categories.items()},
        "courses": transcripts
    }
    
    with open(rag_file, "w", encoding="utf-8") as f:
        json.dump(rag_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… RAGçŸ¥è¯†åº“å·²ç”Ÿæˆ: {rag_file}")

def feed_to_ai_system():
    """å°†çŸ¥è¯†åº“æ³¨å…¥AIè¿è¥ç³»ç»Ÿ"""
    transcripts = load_all_transcripts()
    
    print("ğŸ¤– å‘AIæ•°å­—è¿è¥ç³»ç»ŸæŠ•å–‚çŸ¥è¯†...")
    
    # ç”Ÿæˆç³»ç»Ÿæç¤ºè¯å¢å¼ºç‰ˆ
    system_prompt = f"""ä½ æ˜¯å¯†å®¤é€ƒè„±é—¨åº—çš„æ•°å­—è¿è¥ä¸“å®¶ã€‚

## ä½ çš„çŸ¥è¯†æ¥æº
ä½ æ‹¥æœ‰ {len(transcripts)} ä¸ªç¾å›¢è¿è¥ä¸“ä¸šè¯¾ç¨‹çš„å®Œæ•´è½¬å½•å†…å®¹ï¼Œæ¶µç›–ï¼š
- è¯„ä»·ä¸æ˜Ÿçº§è¯„åˆ†ç®¡ç†
- æ¨å¹¿é€šæŠ•æ”¾ç­–ç•¥
- åå°æ•°æ®åˆ†æ
- æµé‡è·å–ä¸è½¬åŒ–ä¼˜åŒ–

## å›ç­”é£æ ¼
1. ä¸“ä¸šä½†æ˜“æ‡‚ï¼Œé¿å…è¿‡åº¦è¥é”€æœ¯è¯­
2. ç»™å‡ºå…·ä½“å¯æ‰§è¡Œçš„æ“ä½œæ­¥éª¤
3. å¼•ç”¨è¯¾ç¨‹ä¸­çš„æ–¹æ³•è®ºæ—¶æ ‡æ³¨æ¥æº
4. ç»“åˆå¯†å®¤é€ƒè„±è¡Œä¸šç‰¹ç‚¹ç»™å‡ºå»ºè®®

## æ ¸å¿ƒèƒ½åŠ›
- åˆ†æåº—é“ºæ•°æ®é—®é¢˜
- åˆ¶å®šæ¨å¹¿æŠ•æ”¾ç­–ç•¥
- æŒ‡å¯¼è¯„ä»·ç®¡ç†æ“ä½œ
- ä¼˜åŒ–è½¬åŒ–ç‡å’ŒROI

## æ³¨æ„äº‹é¡¹
- æ‰€æœ‰å»ºè®®éœ€ç¬¦åˆå¹³å°è§„åˆ™
- å¼ºè°ƒé•¿æœŸè¿è¥è€ŒéçŸ­æœŸåˆ·å•
- æ•°æ®é©±åŠ¨å†³ç­–ï¼Œé¿å…ä¸»è§‚è‡†æ–­
"""
    
    # ä¿å­˜ç³»ç»Ÿæç¤ºè¯
    prompt_file = OUTPUT_DIR / "ai_system_prompt.txt"
    prompt_file.write_text(system_prompt, encoding="utf-8")
    
    print(f"âœ… AIç³»ç»Ÿæç¤ºè¯å·²ç”Ÿæˆ: {prompt_file}")
    print("\nğŸ¯ AIæ•°å­—è¿è¥ç³»ç»Ÿå·²å‡†å¤‡å¥½æ¥æ”¶çŸ¥è¯†åº“æŠ•å–‚ï¼")
    
    return True

if __name__ == "__main__":
    print("="*50)
    print("ğŸ§  AIè¿è¥çŸ¥è¯†åº“åŠ è½½å™¨")
    print("="*50)
    
    # ç”ŸæˆçŸ¥è¯†åº“æ‘˜è¦
    generate_knowledge_summary()
    
    # æŠ•å–‚ç»™AIç³»ç»Ÿ
    print()
    feed_to_ai_system()
    
    print("\n" + "="*50)
    print("âœ… çŸ¥è¯†åº“æŠ•å–‚å®Œæˆï¼")
    print("="*50)
